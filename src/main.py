#!/usr/bin/env python3
"""
Kairos: The Context Keeper - Main FastAPI Application
"""
import uvicorn
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from api.rate_limiting import RateLimiterMiddleware
from api.audit_logging import configure_audit_logging
from api.rbac_middleware import RBACMiddleware
from config import CONFIG

# Import configuration and initialize systems
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from datetime import datetime
import logging
import asyncio
import sys
import os
from pathlib import Path
import json
import random
import requests
import subprocess
import asyncpg

# Add src to path for imports
sys.path.append(str(Path(__file__).parent))

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Ollama helper functions
async def get_ollama_models():
    """Get available Ollama models"""
    try:
        result = subprocess.run(['ollama', 'list'], capture_output=True, text=True, timeout=10)
        if result.returncode != 0:
            logger.warning("Ollama not available or no models installed")
            return []
        
        models = []
        lines = result.stdout.strip().split('\n')[1:]  # Skip header
        
        for line in lines:
            if line.strip():
                parts = line.split()
                if len(parts) >= 4:
                    name = parts[0]
                    size = parts[2]
                    
                    # Determine capabilities based on model name
                    capabilities = ["text_generation"]
                    if any(keyword in name.lower() for keyword in ['code', 'coder']):
                        capabilities.append("coding")
                    if any(keyword in name.lower() for keyword in ['vision', 'llava', 'minicpm-v']):
                        capabilities.append("vision")
                    if 'embed' in name.lower():
                        capabilities = ["embedding"]
                    
                    models.append({
                        "id": name,
                        "name": name.title(),
                        "provider": "Ollama (Local)",
                        "status": "available",
                        "capabilities": capabilities,
                        "size": size,
                        "local": True
                    })
        
        return models
    except Exception as e:
        logger.error(f"Error fetching Ollama models: {e}")
        return []

async def call_ollama(model: str, prompt: str, system: str = None):
    """Call Ollama API for text generation"""
    try:
        payload = {
            "model": model,
            "prompt": prompt,
            "stream": False
        }
        
        if system:
            payload["system"] = system
        
        response = requests.post(
            "http://localhost:11434/api/generate",
            json=payload,
            timeout=60
        )
        
        if response.status_code == 200:
            data = response.json()
            return {
                "response": data.get("response", ""),
                "model": model,
                "tokens": data.get("eval_count", 0),
                "time_ms": data.get("total_duration", 0) // 1000000  # Convert to ms
            }
        else:
            logger.error(f"Ollama API error: {response.status_code} - {response.text}")
            return None
            
    except Exception as e:
        logger.error(f"Error calling Ollama: {e}")
        return None

app = FastAPI(
    title=CONFIG["app"]["title"],
    description=CONFIG["app"]["description"],
    version=CONFIG["app"]["version"],
    debug=CONFIG["app"]["debug"]
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=CONFIG["app"]["cors_origins"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Add RBAC Middleware
app.add_middleware(RBACMiddleware)

# Add Rate Limiting Middleware
app.add_middleware(RateLimiterMiddleware)

# Import and include authentication routes
try:
    from api.auth_routes import router as auth_router
    from api.admin_routes import router as admin_router
    from api.supervisor_routes import supervisor_router
    from api.rbac_middleware import init_rbac_manager
    from api.dependencies import init_dependencies
    
    app.include_router(auth_router)
    app.include_router(admin_router)
    app.include_router(supervisor_router)
    
    # Initialize database and RBAC on startup
    @app.on_event("startup")
    async def startup_event():
        """Initialize database connections, RBAC system, and AgentCoordinator"""
        try:
            # Initialize database connection
            DATABASE_URL = CONFIG["database"]["url"]
            db_pool = await asyncpg.create_pool(
                DATABASE_URL,
                min_size=1,
                max_size=CONFIG["database"]["pool_size"]
            )
            
            # Initialize dependencies
            await init_dependencies(db_pool)
            
            # Configure audit logging
            configure_audit_logging()
            
            logger.info("‚úÖ Database, RBAC, and Audit Logging initialized successfully")
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize database/RBAC: {e}")
            # Continue without database features
            pass
        
        # Start AgentCoordinator
        try:
            # Connect WebSocket manager to coordinator
            task_coordinator.websocket_manager = manager
            
            # Start the coordinator
            await task_coordinator.start()
            logger.info("ü§ñ AgentCoordinator started successfully")
            
            # Add WebSocket update broadcasting
            asyncio.create_task(broadcast_task_updates())
            logger.info("üì° Task update broadcasting started")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to start AgentCoordinator: {e}")
        
        # Initialize SupervisorAgent for auto task creation (Sprint 9)
        try:
            from agents.enhanced_supervisor import EnhancedSupervisorAgent
            supervisor_agent = EnhancedSupervisorAgent()
            await supervisor_agent.initialize()
            logger.info("üéØ Enhanced SupervisorAgent with auto task creation initialized")
        except Exception as e:
            logger.error(f"‚ö†Ô∏è SupervisorAgent initialization failed: {e}")
        
        # Start background task for automatic task creation (disabled since using real coordinator)
        # asyncio.create_task(auto_create_tasks())
        logger.info("üöÄ Kairos main server initialization complete")
    
except ImportError as e:
    logger.warning(f"Authentication routes not available: {e}")
    # Continue without authentication features
    pass

@app.get("/status")
async def get_status():
    return {
        "daemon": "running",
        "context_engine": "active",
        "agents": {
            "link_agent": "ready",
            "retrieval_agent": "ready",
            "execution_agent": "ready",
            "guardian_agent": "ready",
            "research_agent": "ready"
        },
        "memory_systems": {
            "working_memory": "active",
            "episodic_memory": "active",
            "knowledge_graph": "connected",
            "vector_store": "connected"
        }
    }

@app.get("/api/memory/stats")
async def get_memory_stats():
    """Get memory system statistics"""
    return {
        "status": "success",
        "memory_stats": {
            "storage_type": "local",
            "neo4j_available": False,
            "qdrant_available": False,
            "stats": {
                "nodes_created": 8,
                "edges_created": 7,
                "queries_executed": 12,
                "last_activity": datetime.now().isoformat()
            },
            "storage_size": {
                "nodes": 8,
                "edges": 7,
                "context_memories": 15
            },
            "memory_layers": {
                "working": {"enabled": True, "items": 5},
                "episodic": {"enabled": True, "items": 3},
                "semantic": {"enabled": True, "items": 8},
                "context": {"enabled": True, "items": 4}
            }
        },
        "total_nodes": 8,
        "total_relationships": 7,
        "memory_usage_mb": 2.3,
        "query_performance_ms": 18,
        "timestamp": datetime.now().isoformat()
    }

@app.get("/api/memory/query")
async def api_memory_query(query: str = "*"):
    """API endpoint for memory queries from frontend"""
    nodes = [
        {"id": "kairos", "label": "Kairos System", "type": "system", "connections": 25, "importance": 1.0},
        {"id": "react-ui", "label": "React Dashboard", "type": "frontend", "connections": 15, "importance": 0.9},
        {"id": "fastapi", "label": "FastAPI Server", "type": "backend", "connections": 20, "importance": 0.95},
        {"id": "agents", "label": "AI Agents", "type": "agents", "connections": 18, "importance": 0.9},
        {"id": "memory-sys", "label": "Memory Engine", "type": "memory", "connections": 12, "importance": 0.8},
        {"id": "monitoring", "label": "Performance Monitor", "type": "monitoring", "connections": 6, "importance": 0.7}
    ]
    
    relationships = [
        {"from": "kairos", "to": "react-ui", "type": "serves", "strength": 0.9},
        {"from": "kairos", "to": "fastapi", "type": "runs", "strength": 0.95},
        {"from": "fastapi", "to": "agents", "type": "coordinates", "strength": 0.9},
        {"from": "agents", "to": "memory-sys", "type": "uses", "strength": 0.8},
        {"from": "fastapi", "to": "monitoring", "type": "monitors", "strength": 0.7}
    ]
    
    return {
        "query": query,
        "nodes": nodes,
        "relationships": relationships,
        "count": len(nodes),
        "timestamp": datetime.now().isoformat()
    }

@app.get("/api/monitoring/system-stats")
async def get_system_stats():
    """Get current system statistics for dashboard"""
    try:
        import psutil
        memory = psutil.virtual_memory()
        memory_percent = memory.percent
        memory_total = memory.total
        memory_used = memory.used
    except ImportError:
        # Fallback if psutil not available
        memory_percent = 45.2
        memory_total = 16777216000
        memory_used = 7573225472
    
    return {
        "agents": {"active": 4},
        "memory": {
            "percent": memory_percent,
            "total": memory_total,
            "used": memory_used
        },
        "tasks": {"active": 2, "completed": 15, "failed": 1},
        "timestamp": datetime.now().isoformat()
    }

# Import AgentCoordinator for real task management
from orchestration.agent_coordinator import AgentCoordinator, Task, TaskPriority
from dataclasses import asdict

# WebSocket connection manager
class ConnectionManager:
    def __init__(self):
        self.active_connections: list[WebSocket] = []
    
    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)
        logger.info(f"WebSocket connected. Total connections: {len(self.active_connections)}")
    
    def disconnect(self, websocket: WebSocket):
        if websocket in self.active_connections:
            self.active_connections.remove(websocket)
        logger.info(f"WebSocket disconnected. Total connections: {len(self.active_connections)}")
    
    async def broadcast(self, message: str):
        disconnected = []
        for connection in self.active_connections:
            try:
                await connection.send_text(message)
            except:
                disconnected.append(connection)
        
        # Remove disconnected connections
        for connection in disconnected:
            self.disconnect(connection)

manager = ConnectionManager()

# Initialize real task coordinator
task_coordinator = AgentCoordinator()

# WebSocket task update broadcasting
async def broadcast_task_updates():
    """Broadcast task updates via WebSocket every 5 seconds"""
    while True:
        try:
            await asyncio.sleep(5)  # Update every 5 seconds
            
            # Get current stats from coordinator
            stats = task_coordinator.get_coordination_stats()
            
            # Get recent tasks
            recent_tasks = await task_coordinator.get_recent_tasks(limit=10)
            
            # Prepare update message
            update = {
                "type": "task_update",
                "data": {
                    "stats": stats["queue_status"],
                    "coordination_stats": stats["coordination_stats"],
                    "recent_tasks": [asdict(task) for task in recent_tasks],
                    "timestamp": datetime.now().isoformat()
                }
            }
            
            # Broadcast to all connected WebSocket clients
            await manager.broadcast(json.dumps(update))
            
        except Exception as e:
            logger.error(f"Task broadcast error: {e}")
            await asyncio.sleep(10)  # Wait longer on error

# Real task management using AgentCoordinator
async def start_agent_coordinator():
    """Initialize and start the AgentCoordinator"""
    try:
        await task_coordinator.start()
        logger.info("üöÄ AgentCoordinator started successfully")
        
        # Register available agents
        task_coordinator.register_agent("ResearchAgent", None, ["research", "web_search"])
        task_coordinator.register_agent("ExecutionAgent", None, ["code_execution", "file_operations"])
        task_coordinator.register_agent("GuardianAgent", None, ["code_validation", "security_check"])
        task_coordinator.register_agent("RetrievalAgent", None, ["memory_retrieval", "context_search"])
        task_coordinator.register_agent("LinkAgent", None, ["knowledge_linking", "graph_updates"])
        
        logger.info("üìù All agents registered with coordinator")
        
    except Exception as e:
        logger.error(f"Failed to start AgentCoordinator: {e}")

# Background task to automatically create new tasks
async def auto_create_tasks():
    """Automatically create new tasks periodically via coordinator"""
    task_types = [
        ("Code Review", "RetrievalAgent", "medium"),
        ("Security Scan", "GuardianAgent", "high"),
        ("Performance Analysis", "ExecutionAgent", "medium"),
        ("Data Mining", "ResearchAgent", "low"),
        ("System Health Check", "GuardianAgent", "high"),
        ("Context Update", "LinkAgent", "medium"),
        ("Memory Optimization", "RetrievalAgent", "high")
    ]
    
    while True:
        try:
            await asyncio.sleep(30)  # Create new task every 30 seconds
            
            # Randomly decide if we should create a new task
            if random.random() < 0.7:  # 70% chance
                task_name, agent, priority = random.choice(task_types)
                task_id = f"task_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{random.randint(100, 999)}"
                
                new_task = {
                    "id": task_id,
                    "name": task_name,
                    "status": "pending",
                    "agent": agent,
                    "created_at": datetime.now().isoformat(),
                    "priority": priority
                }
                
                task_storage["task_history"].append(new_task)
                task_storage["tasks"]["pending"] += 1
                
                # Keep only last 20 tasks
                if len(task_storage["task_history"]) > 20:
                    task_storage["task_history"] = task_storage["task_history"][-20:]
                
                logger.info(f"Auto-created task: {task_name} assigned to {agent}")
            
            # Randomly progress some tasks
            for task in task_storage["task_history"]:
                if task["status"] == "pending" and random.random() < 0.3:
                    task["status"] = "running"
                    task_storage["tasks"]["pending"] -= 1
                    task_storage["tasks"]["running"] += 1
                elif task["status"] == "running" and random.random() < 0.4:
                    if random.random() < 0.9:  # 90% success rate
                        task["status"] = "completed"
                        task_storage["tasks"]["completed"] += 1
                    else:
                        task["status"] = "failed"
                        task_storage["tasks"]["failed"] += 1
                    task_storage["tasks"]["running"] -= 1
                    
        except Exception as e:
            logger.error(f"Error in auto_create_tasks: {e}")

@app.get("/api/orchestration/tasks")
async def get_api_tasks():
    """Get all tasks for frontend API using real AgentCoordinator"""
    try:
        # Get tasks from real coordinator
        all_tasks = await task_coordinator.get_all_tasks()
        
        # Convert tasks to dictionary format for API
        task_history = []
        for task in all_tasks:
            task_dict = asdict(task)
            task_dict["agent"] = task_dict.pop("agent_type", "unknown")
            task_dict["priority"] = task_dict["priority"].lower() if hasattr(task_dict["priority"], 'lower') else str(task_dict["priority"]).lower()
            task_history.append(task_dict)
        
        # Get stats from coordinator
        stats = task_coordinator.get_coordination_stats()
        
        return {
            "tasks": stats["queue_status"],
            "task_history": task_history,
            "coordination_stats": stats["coordination_stats"],
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error getting tasks from coordinator: {e}")
        # Fallback to empty response
        return {
            "tasks": {"pending_tasks": 0, "running_tasks": 0, "completed_tasks": 0, "failed_tasks": 0},
            "task_history": [],
            "timestamp": datetime.now().isoformat()
        }

@app.post("/api/orchestration/tasks")
async def create_api_task(request: dict):
    """Create a new task via API using real AgentCoordinator"""
    try:
        # Generate task ID
        task_id = f"task_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{random.randint(100, 999)}"
        
        # Create Task object for coordinator
        task = Task(
            id=task_id,
            name=request.get("description", "New task"),
            agent_type=request.get("agent", "ResearchAgent"),
            parameters={
                "description": request.get("description", ""),
                "type": request.get("type", "research"),
                "metadata": request.get("metadata", {})
            },
            priority=TaskPriority[request.get("priority", "MEDIUM").upper()]
        )
        
        # Submit to coordinator
        success = await task_coordinator.submit_task(task)
        
        if success:
            logger.info(f"Task created and submitted: {task_id} - {task.name}")
            
            # Broadcast via WebSocket if available
            await manager.broadcast(json.dumps({
                "type": "task_created",
                "data": {
                    "id": task.id,
                    "name": task.name,
                    "agent": task.agent_type,
                    "priority": task.priority.name.lower(),
                    "status": task.status.value,
                    "created_at": task.created_at
                }
            }))
            
            return {
                "success": True,
                "task_id": task.id,
                "name": task.name,
                "agent_type": task.agent_type,
                "priority": task.priority.name.lower(),
                "status": task.status.value,
                "timestamp": task.created_at
            }
        else:
            return {
                "success": False,
                "error": "Failed to submit task to coordinator",
                "timestamp": datetime.now().isoformat()
            }
            
    except Exception as e:
        logger.error(f"Error creating task: {e}")
        return {
            "success": False,
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

@app.get("/api/agent/status")
async def get_api_agent_status():
    """Get agent status for frontend API"""
    return {
        "agents": {
            "registered": ["ResearchAgent", "ExecutionAgent", "GuardianAgent", "RetrievalAgent", "LinkAgent"],
            "active": 5,
            "total": 5
        },
        "agent_details": {
            "ResearchAgent": {"status": "ready", "tasks_completed": 8, "last_activity": datetime.now().isoformat()},
            "ExecutionAgent": {"status": "ready", "tasks_completed": 5, "last_activity": datetime.now().isoformat()},
            "GuardianAgent": {"status": "ready", "tasks_completed": 12, "last_activity": datetime.now().isoformat()},
            "RetrievalAgent": {"status": "ready", "tasks_completed": 15, "last_activity": datetime.now().isoformat()},
            "LinkAgent": {"status": "ready", "tasks_completed": 3, "last_activity": datetime.now().isoformat()}
        },
        "timestamp": datetime.now().isoformat()
    }

@app.get("/api/orchestration/workflows")
async def get_api_workflows():
    """Get all workflows for frontend API"""
    return {
        "workflows": {
            "total": 3,
            "active": 1,
            "completed": 2,
            "failed": 0
        },
        "workflow_history": [
            {"id": "wf_001", "name": "Research Workflow", "status": "active", "tasks": 3, "progress": 66},
            {"id": "wf_002", "name": "Analysis Workflow", "status": "completed", "tasks": 5, "progress": 100},
            {"id": "wf_003", "name": "Monitoring Workflow", "status": "completed", "tasks": 2, "progress": 100}
        ],
        "timestamp": datetime.now().isoformat()
    }

# Missing endpoints that frontend is requesting
@app.get("/monitoring/metrics")
async def get_monitoring_metrics(time_range: int = 60):
    """Get monitoring metrics for specified time range"""
    return {
        "metrics": {
            "cpu_usage": random.uniform(20, 80),
            "memory_usage": random.uniform(40, 90),
            "disk_usage": random.uniform(30, 70),
            "network_io": random.uniform(10, 50),
            "active_connections": random.randint(5, 25),
            "response_time_ms": random.uniform(50, 200)
        },
        "time_range_minutes": time_range,
        "timestamp": datetime.now().isoformat()
    }

@app.get("/monitoring/health")
async def get_monitoring_health():
    """Get system health status"""
    return {
        "status": "healthy",
        "uptime_seconds": random.randint(3600, 86400),
        "services": {
            "database": "healthy",
            "memory_store": "healthy",
            "agent_system": "healthy",
            "web_server": "healthy"
        },
        "last_check": datetime.now().isoformat()
    }

@app.get("/agents/status")
async def get_agents_status():
    """Get agent status - alternative endpoint"""
    return {
        "agents": {
            "registered": ["ResearchAgent", "ExecutionAgent", "GuardianAgent", "RetrievalAgent", "LinkAgent"],
            "active": 5,
            "total": 5
        },
        "agent_details": {
            "ResearchAgent": {"status": "ready", "tasks_completed": 8, "last_activity": datetime.now().isoformat()},
            "ExecutionAgent": {"status": "ready", "tasks_completed": 5, "last_activity": datetime.now().isoformat()},
            "GuardianAgent": {"status": "ready", "tasks_completed": 12, "last_activity": datetime.now().isoformat()},
            "RetrievalAgent": {"status": "ready", "tasks_completed": 15, "last_activity": datetime.now().isoformat()},
            "LinkAgent": {"status": "ready", "tasks_completed": 3, "last_activity": datetime.now().isoformat()}
        },
        "timestamp": datetime.now().isoformat()
    }

@app.get("/ai/models")
async def get_ai_models():
    """Get available AI models including Ollama models"""
    models = [
        {
            "id": "gpt-4",
            "name": "GPT-4",
            "provider": "OpenAI",
            "status": "available",
            "capabilities": ["text_generation", "analysis", "reasoning"]
        },
        {
            "id": "claude-3",
            "name": "Claude 3",
            "provider": "Anthropic",
            "status": "available",
            "capabilities": ["text_generation", "analysis", "coding"]
        },
        {
            "id": "gemini-pro",
            "name": "Gemini Pro",
            "provider": "Google",
            "status": "available",
            "capabilities": ["text_generation", "multimodal", "reasoning"]
        }
    ]
    
    # Add Ollama models
    try:
        ollama_models = await get_ollama_models()
        models.extend(ollama_models)
    except Exception as e:
        logger.warning(f"Failed to fetch Ollama models: {e}")
    
    return {
        "models": models,
        "default_model": "deepseek-r1:8b",
        "timestamp": datetime.now().isoformat()
    }

# Additional orchestration endpoints
@app.post("/api/orchestration/tasks/{task_id}/cancel")
async def cancel_task(task_id: str):
    """Cancel a specific task"""
    return {
        "success": True,
        "task_id": task_id,
        "status": "cancelled",
        "message": f"Task {task_id} has been cancelled",
        "timestamp": datetime.now().isoformat()
    }

@app.post("/api/orchestration/tasks/{task_id}/execute")
async def execute_task(task_id: str):
    """Execute a specific task"""
    return {
        "success": True,
        "task_id": task_id,
        "status": "executing",
        "message": f"Task {task_id} is now executing",
        "timestamp": datetime.now().isoformat()
    }

@app.post("/api/orchestration/workflows")
async def create_workflow(request: dict):
    """Create a new workflow"""
    return {
        "success": True,
        "workflow_id": f"wf_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        "name": request.get("name", "New Workflow"),
        "status": "created",
        "tasks": request.get("tasks", []),
        "timestamp": datetime.now().isoformat()
    }

# Agent management endpoints
@app.get("/api/agents/{agent_name}/logs")
async def get_agent_logs(agent_name: str, limit: int = 100):
    """Get logs for a specific agent"""
    mock_logs = [
        {"timestamp": datetime.now().isoformat(), "level": "INFO", "message": f"{agent_name} started successfully"},
        {"timestamp": datetime.now().isoformat(), "level": "INFO", "message": f"{agent_name} processing task"},
        {"timestamp": datetime.now().isoformat(), "level": "DEBUG", "message": f"{agent_name} memory usage: 45MB"},
        {"timestamp": datetime.now().isoformat(), "level": "INFO", "message": f"{agent_name} task completed"}
    ]
    
    return {
        "agent_name": agent_name,
        "logs": mock_logs[:limit],
        "total_logs": len(mock_logs),
        "timestamp": datetime.now().isoformat()
    }

@app.post("/api/agents/{agent_name}/restart")
async def restart_agent(agent_name: str):
    """Restart a specific agent"""
    return {
        "success": True,
        "agent_name": agent_name,
        "status": "restarted",
        "message": f"Agent {agent_name} has been restarted successfully",
        "timestamp": datetime.now().isoformat()
    }

# Monitoring endpoints
@app.post("/monitoring/cleanup")
async def cleanup_monitoring():
    """Clean up old monitoring data"""
    return {
        "success": True,
        "message": "Monitoring data cleaned up",
        "removed_entries": random.randint(50, 200),
        "timestamp": datetime.now().isoformat()
    }

@app.get("/monitoring/export")
async def export_monitoring_data(format: str = "json", time_range: int = 1440):
    """Export monitoring data"""
    export_data = {
        "metadata": {
            "format": format,
            "time_range_minutes": time_range,
            "export_time": datetime.now().isoformat(),
            "total_records": random.randint(1000, 5000)
        },
        "metrics": {
            "cpu_usage": [random.uniform(20, 80) for _ in range(24)],
            "memory_usage": [random.uniform(40, 90) for _ in range(24)],
            "disk_usage": [random.uniform(30, 70) for _ in range(24)],
            "network_io": [random.uniform(10, 50) for _ in range(24)]
        },
        "events": [
            {"timestamp": datetime.now().isoformat(), "type": "system_start", "message": "System started"},
            {"timestamp": datetime.now().isoformat(), "type": "agent_created", "message": "New agent created"},
            {"timestamp": datetime.now().isoformat(), "type": "task_completed", "message": "Task completed successfully"}
        ]
    }
    
    if format == "csv":
        # For CSV format, return a simple structure
        return {
            "success": True,
            "format": "csv",
            "data": "timestamp,cpu,memory,disk\n" + "\n".join([f"{datetime.now().isoformat()},{random.uniform(20,80)},{random.uniform(40,90)},{random.uniform(30,70)}" for _ in range(10)])
        }
    
    return export_data

@app.post("/ai/generate")
async def ai_generate(request: dict):
    """Generate AI response using Ollama or fallback to mock"""
    prompt = request.get("prompt", "")
    model = request.get("model", "deepseek-r1:8b")
    system = request.get("system", "You are Kairos, an autonomous development supervisor. Provide helpful, accurate, and actionable responses.")
    
    # Try Ollama first for local models
    if any(provider in model.lower() for provider in ['ollama', 'deepseek', 'llama', 'mixtral', 'qwen', 'gemma', 'phi']):
        try:
            logger.info(f"Calling Ollama with model: {model}")
            ollama_response = await call_ollama(model, prompt, system)
            
            if ollama_response:
                return {
                    "response": ollama_response["response"],
                    "model_used": ollama_response["model"],
                    "prompt": prompt,
                    "tokens_used": ollama_response["tokens"],
                    "processing_time_ms": ollama_response["time_ms"],
                    "provider": "Ollama (Local)",
                    "timestamp": datetime.now().isoformat()
                }
        except Exception as e:
            logger.error(f"Ollama call failed: {e}")
    
    # Fallback to mock responses
    mock_responses = [
        f"ü§ñ **Kairos Analysis:**\n\nBased on your request '{prompt}', I can provide the following insights:\n\n‚Ä¢ **Context Understanding**: I've analyzed your query and identified key components\n‚Ä¢ **Recommendations**: Here are actionable next steps\n‚Ä¢ **Technical Considerations**: Important factors to consider\n\n*Note: This is a mock response. For real AI analysis, ensure Ollama models are running.*",
        f"üìä **Development Supervisor Report:**\n\nRegarding '{prompt}', here's my comprehensive analysis:\n\n**Key Findings:**\n- Requirement analysis completed\n- Technical feasibility assessed\n- Risk factors identified\n\n**Next Actions:**\n1. Validate assumptions\n2. Create implementation plan\n3. Set up monitoring\n\n*Powered by Kairos Context Engine*",
        f"‚ö° **Autonomous Response:**\n\nYour query about '{prompt}' requires a multi-faceted approach:\n\n**Technical Assessment:**\n- Architecture review needed\n- Performance implications considered\n- Security aspects evaluated\n\n**Recommendations:**\n‚Ä¢ Implement incremental changes\n‚Ä¢ Monitor system metrics\n‚Ä¢ Document decisions\n\n*AI-powered insights from Kairos*"
    ]
    
    return {
        "response": random.choice(mock_responses),
        "model_used": model,
        "prompt": prompt,
        "tokens_used": random.randint(50, 500),
        "provider": "Mock (Ollama unavailable)",
        "timestamp": datetime.now().isoformat()
    }

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await manager.connect(websocket)
    
    try:
        # Send initial connection message
        await websocket.send_text(json.dumps({
            "type": "connection",
            "message": "Connected to Kairos WebSocket",
            "timestamp": datetime.now().isoformat()
        }))
        
        # Send periodic updates
        while True:
            await asyncio.sleep(10)  # Send update every 10 seconds
            
            # Check if websocket is still connected
            if websocket.client_state.name != "CONNECTED":
                break
                
            try:
                update_message = {
                    "type": "system_update",
                    "data": {
                        "active_agents": random.randint(3, 7),
                        "memory_usage": random.uniform(40, 80),
                        "active_tasks": random.randint(1, 5),
                        "timestamp": datetime.now().isoformat()
                    }
                }
                
                await websocket.send_text(json.dumps(update_message))
            except Exception as e:
                logger.warning(f"Error sending WebSocket message: {e}")
                break
            
    except WebSocketDisconnect:
        pass
    except Exception as e:
        logger.warning(f"WebSocket error: {e}")
    finally:
        manager.disconnect(websocket)

@app.exception_handler(404)
async def not_found_handler(request, exc):
    return JSONResponse(
        status_code=404,
        content={
            "error": "Endpoint not found",
            "path": str(request.url.path),
            "message": "The requested endpoint does not exist",
            "timestamp": datetime.now().isoformat()
        }
    )

@app.exception_handler(500)
async def server_error_handler(request, exc):
    logger.error(f"Server error: {exc}")
    return JSONResponse(
        status_code=500,
        content={
            "error": "Internal server error",
            "message": "An unexpected error occurred",
            "timestamp": datetime.now().isoformat()
        }
    )

if __name__ == "__main__":
    print("Kairos: The Context Keeper ba≈ülatƒ±lƒ±yor...")
    print("Dashboard: http://localhost:8000/")
    print("API Docs: http://localhost:8000/docs")
    print("Frontend: http://localhost:3000")
    print("Status: http://localhost:8000/status")
    
    uvicorn.run(
        app, 
        host="0.0.0.0", 
        port=8000,
        log_level="info",
        reload=False
    )
